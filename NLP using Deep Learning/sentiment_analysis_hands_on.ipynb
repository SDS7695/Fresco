{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this hands-on you will be building a sentiment classifier on for movie reviews using word vectors and LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all the necessary packages in the below cell as and when you require"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.datasets.imdb import get_word_index\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading the dataset.\n",
    "- Keras has a built in function to download movie review available in imdb. \n",
    "- Each words in the review are represented by their unique index and the labels are in binary format representing positive or negative reviews\n",
    "- The necessary code to download the dataset has been written for you.\n",
    "- The variable **word_to_id** is a dictionary containing words and their corresponding ids\n",
    "- Run the below cell to download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 4s 0us/step\n",
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 1s 1us/step\n",
      "word to id fist five samples {'fawn': 34701, 'tsukino': 52006, 'nunnery': 52007, 'sonja': 16816, 'vani': 63951}\n",
      "\n",
      "\n",
      "sample input\n",
      " [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n",
      "\n",
      "\n",
      "target output 1\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5000\n",
    "(X_train, Y_train), (X_test, Y_test) = imdb.load_data(num_words=vocab_size)\n",
    "word_to_id = get_word_index()\n",
    "print(\"word to id fist five samples {}\".format({key:value for key, value in zip(list(word_to_id.keys())[:5], list(word_to_id.values())[:5])}))\n",
    "print(\"\\n\")\n",
    "print(\"sample input\\n\", X_train[0])\n",
    "print('\\n')\n",
    "print(\"target output\", Y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Each review in the dataset has some special tokens such as\n",
    "    - <START> : to identify the start of the sentence\n",
    "    - <UNK> : If some words are not identified in the vocabulary\n",
    "    - <PAD> : The value to be filled if sequence requires padding\n",
    "### Task 1:\n",
    "    - offset the word_to_id dictionary by three values such that 0,1,2 represents START, UNK, PAD respectively\n",
    "    - Once you perform the above step reverse the word_to_id dictionary to represent ids as keys and words as values. Assign the resulting dictionary to id_to_word variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_FROM = 3\n",
    "word_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}\n",
    "\n",
    "word_to_id[\"PAD\"] = 0\n",
    "word_to_id[\"START\"] =1 \n",
    "word_to_id[\"UNK\"] = 2\n",
    "id_to_word = {value:key for key,value in word_to_id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the below code to view the first review in training samples\n",
    "\n",
    "### Expected output\n",
    "START this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert UNK is an amazing actor and now the same being director UNK father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for UNK and would recommend it to everyone to watch and the fly UNK was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also UNK to the two little UNK that played the UNK of norman and paul they were just brilliant children are often left out of the UNK list i think because the stars that play them all grown up are such a big UNK for the whole film but these children are amazing and should be UNK for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was UNK with us all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert UNK is an amazing actor and now the same being director UNK father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for UNK and would recommend it to everyone to watch and the fly UNK was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also UNK to the two little UNK that played the UNK of norman and paul they were just brilliant children are often left out of the UNK list i think because the stars that play them all grown up are such a big UNK for the whole film but these children are amazing and should be UNK for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was UNK with us all\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join([id_to_word[i] for i in X_train[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since each movie reviews are of variable lengths in terms of number of words, so it is necessay to fix the review lenght to few words say upto first 500 words.\n",
    "### Task 3\n",
    "   - For each of the samples of X_train and X_test sample upto first 500 words\n",
    "   - If reviews are less than 500 words pad the sequence with zeros in the beginning to make up the length upto 500\n",
    "   - Assign the padded sequence to X_train_pad and X_test_pad variables for train and test smaples respectively\n",
    "   \n",
    "   [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
    "    0    0    1   14   22   16   43  530  973 1622 1385   65  458 4468\n",
    "   66 3941    4  173   36  256    5   25  100   43  838  112   50  670\n",
    "    2    9   35  480  284    5  150    4  172  112  167    2  336  385\n",
    "   39    4  172 4536 1111   17  546   38   13  447    4  192   50   16\n",
    "    6  147 2025   19   14   22    4 1920 4613  469    4   22   71   87\n",
    "   12   16   43  530   38   76   15   13 1247    4   22   17  515   17\n",
    "   12   16  626   18    2    5   62  386   12    8  316    8  106    5\n",
    "    4 2223    2   16  480   66 3785   33    4  130   12   16   38  619\n",
    "    5   25  124   51   36  135   48   25 1415   33    6   22   12  215\n",
    "   28   77   52    5   14  407   16   82    2    8    4  107  117    2\n",
    "   15  256    4    2    7 3766    5  723   36   71   43  530  476   26\n",
    "  400  317   46    7    4    2 1029   13  104   88    4  381   15  297\n",
    "   98   32 2071   56   26  141    6  194    2   18    4  226   22   21\n",
    "  134  476   26  480    5  144   30    2   18   51   36   28  224   92\n",
    "   25  104    4  226   65   16   38 1334   88   12   16  283    5   16\n",
    " 4472  113  103   32   15   16    2   19  178   32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    1   14   22   16   43  530  973 1622 1385   65  458 4468\n",
      "   66 3941    4  173   36  256    5   25  100   43  838  112   50  670\n",
      "    2    9   35  480  284    5  150    4  172  112  167    2  336  385\n",
      "   39    4  172 4536 1111   17  546   38   13  447    4  192   50   16\n",
      "    6  147 2025   19   14   22    4 1920 4613  469    4   22   71   87\n",
      "   12   16   43  530   38   76   15   13 1247    4   22   17  515   17\n",
      "   12   16  626   18    2    5   62  386   12    8  316    8  106    5\n",
      "    4 2223    2   16  480   66 3785   33    4  130   12   16   38  619\n",
      "    5   25  124   51   36  135   48   25 1415   33    6   22   12  215\n",
      "   28   77   52    5   14  407   16   82    2    8    4  107  117    2\n",
      "   15  256    4    2    7 3766    5  723   36   71   43  530  476   26\n",
      "  400  317   46    7    4    2 1029   13  104   88    4  381   15  297\n",
      "   98   32 2071   56   26  141    6  194    2   18    4  226   22   21\n",
      "  134  476   26  480    5  144   30    2   18   51   36   28  224   92\n",
      "   25  104    4  226   65   16   38 1334   88   12   16  283    5   16\n",
      " 4472  113  103   32   15   16    2   19  178   32]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "X_train_pad = pad_sequences(X_train,maxlen=500)\n",
    "X_test_pad =  pad_sequences(X_test,maxlen=500)\n",
    "print(X_train_pad[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using keras [Sequential](https://keras.io/getting-started/sequential-model-guide/) to build an LSTM model using the below specifications\n",
    "- Add an embedding layer( the look up table) such that vacabulary size is 5000 and each word in the vocabulary is 32 dimension vector\n",
    "- Add an LSTM layer with 100 hidden nodes\n",
    "- Add a final sigmoid activation layer\n",
    "- Use adam optimizer and binary cross entropy loss, and metrics as accuracy\n",
    "\n",
    "### Expected Output:\n",
    "\n",
    "#### Layer (type)                 Output Shape              Param #   \n",
    "=================================================================  \n",
    "embedding_5 (Embedding)      (None, None, 32)          160000      \n",
    "_________________________________________________________________  \n",
    "lstm_5 (LSTM)                (None, 100)               53200     \n",
    "_________________________________________________________________  \n",
    "#### dense_5 (Dense)              (None, 1)                 101       \n",
    "=================================================================    \n",
    "Total params: 213,301   \n",
    "Trainable params: 213,301   \n",
    "Non-trainable params: 0  \n",
    "_________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 32)          160000    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "embedding_vector_length = 32 \n",
    "model = Sequential() \n",
    "model.add(Embedding(vocab_size,embedding_vector_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "print(model.summary()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model with X_train_pad and Y_train as train data and X_test_pad, Y_test as Validation set\n",
    "    - set the number of epochs to 3\n",
    "    - set batch size to 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 518s 21ms/step - loss: 0.4557 - acc: 0.7753 - val_loss: 0.3559 - val_acc: 0.8507\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 533s 21ms/step - loss: 0.3593 - acc: 0.8442 - val_loss: 0.3524 - val_acc: 0.8512\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc4db1742e8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Start code here\n",
    "model.fit(X_train_pad,Y_train,validation_data=[X_test_pad,Y_test],epochs=2,batch_size=64)\n",
    "###End code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the below cell to run the model prediction on custom samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i really liked the movie and had fun. Sentiment: 0.37733072\n",
      "this movie was terrible and bad. Sentiment: 0.007098629\n",
      "1000/1000 [==============================] - 4s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "bad = \"this movie was terrible and bad\"\n",
    "good = \"i really liked the movie and had fun\"\n",
    "for review in [good,bad]:\n",
    "    tmp = []\n",
    "    for word in review.split(\" \"):\n",
    "        tmp.append(word_to_id[word])\n",
    "    tmp_padded = pad_sequences([tmp], maxlen=500) \n",
    "    print(\"%s. Sentiment: %s\" % (review,model.predict(np.array([tmp_padded][0]))[0][0]))\n",
    "\n",
    "accuracy = model.evaluate(X_test_pad[:1000], Y_test[:1000])[1]\n",
    "with open('output.txt', 'w') as file:\n",
    "    file.write(str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
